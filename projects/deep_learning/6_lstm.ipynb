{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:1000])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character o in batch 0\n",
      "Character w in batch 1\n",
      "Character l in batch 2\n",
      "Character   in batch 3\n",
      "Character m in batch 4\n",
      "Character h in batch 5\n",
      "Character y in batch 6\n",
      "Character a in batch 7\n",
      "Character t in batch 8\n",
      "Character m in batch 9\n",
      "Character n in batch 10\n",
      "Character h in batch 11\n",
      "Character e in batch 12\n",
      "Character e in batch 13\n",
      "Character o in batch 14\n",
      "Character y in batch 15\n",
      "Character o in batch 16\n",
      "Character a in batch 17\n",
      "Character   in batch 18\n",
      "Character a in batch 19\n",
      "Character i in batch 20\n",
      "Character   in batch 21\n",
      "Character t in batch 22\n",
      "Character d in batch 23\n",
      "Character f in batch 24\n",
      "Character a in batch 25\n",
      "Character e in batch 26\n",
      "Character e in batch 27\n",
      "Character a in batch 28\n",
      "Character r in batch 29\n",
      "Character i in batch 30\n",
      "Character o in batch 31\n",
      "Character a in batch 32\n",
      "Character g in batch 33\n",
      "Character i in batch 34\n",
      "Character r in batch 35\n",
      "Character c in batch 36\n",
      "Character a in batch 37\n",
      "Character   in batch 38\n",
      "Character m in batch 39\n",
      "Character t in batch 40\n",
      "Character u in batch 41\n",
      "Character e in batch 42\n",
      "Character o in batch 43\n",
      "Character o in batch 44\n",
      "Character s in batch 45\n",
      "Character k in batch 46\n",
      "Character e in batch 47\n",
      "Character w in batch 48\n",
      "Character e in batch 49\n",
      "Character t in batch 50\n",
      "Character e in batch 51\n",
      "Character   in batch 52\n",
      "Character i in batch 53\n",
      "Character t in batch 54\n",
      "Character d in batch 55\n",
      "Character t in batch 56\n",
      "Character e in batch 57\n",
      "Character f in batch 58\n",
      "Character d in batch 59\n",
      "Character t in batch 60\n",
      "Character a in batch 61\n",
      "Character a in batch 62\n",
      "Character s in batch 63\n",
      "Character   in batch 0\n",
      "Character n in batch 0\n",
      "Character h in batch 1\n",
      "Character l in batch 2\n",
      "Character a in batch 3\n",
      "Character a in batch 4\n",
      "Character e in batch 5\n",
      "Character   in batch 6\n",
      "Character y in batch 7\n",
      "Character i in batch 8\n",
      "Character i in batch 9\n",
      "Character e in batch 10\n",
      "Character e in batch 11\n",
      "Character   in batch 12\n",
      "Character b in batch 13\n",
      "Character   in batch 14\n",
      "Character e in batch 15\n",
      "Character r in batch 16\n",
      "Character   in batch 17\n",
      "Character t in batch 18\n",
      "Character r in batch 19\n",
      "Character t in batch 20\n",
      "Character a in batch 21\n",
      "Character i in batch 22\n",
      "Character y in batch 23\n",
      "Character   in batch 24\n",
      "Character t in batch 25\n",
      "Character   in batch 26\n",
      "Character n in batch 27\n",
      "Character m in batch 28\n",
      "Character v in batch 29\n",
      "Character o in batch 30\n",
      "Character   in batch 31\n",
      "Character   in batch 32\n",
      "Character h in batch 33\n",
      "Character n in batch 34\n",
      "Character o in batch 35\n",
      "Character a in batch 36\n",
      "Character s in batch 37\n",
      "Character d in batch 38\n",
      "Character o in batch 39\n",
      "Character   in batch 40\n",
      "Character   in batch 41\n",
      "Character   in batch 42\n",
      "Character   in batch 43\n",
      "Character f in batch 44\n",
      "Character   in batch 45\n",
      "Character l in batch 46\n",
      "Character r in batch 47\n",
      "Character s in batch 48\n",
      "Character t in batch 49\n",
      "Character h in batch 50\n",
      "Character t in batch 51\n",
      "Character s in batch 52\n",
      "Character s in batch 53\n",
      "Character i in batch 54\n",
      "Character   in batch 55\n",
      "Character h in batch 56\n",
      "Character n in batch 57\n",
      "Character e in batch 58\n",
      "Character u in batch 59\n",
      "Character r in batch 60\n",
      "Character t in batch 61\n",
      "Character p in batch 62\n",
      "Character i in batch 63\n",
      "Character s in batch 0\n",
      "Character e in batch 1\n",
      "Character e in batch 2\n",
      "Character b in batch 3\n",
      "Character r in batch 4\n",
      "Character l in batch 5\n",
      "Character a in batch 6\n",
      "Character   in batch 7\n",
      "Character o in batch 8\n",
      "Character g in batch 9\n",
      "Character w in batch 10\n",
      "Character   in batch 11\n",
      "Character l in batch 12\n",
      "Character e in batch 13\n",
      "Character b in batch 14\n",
      "Character r in batch 15\n",
      "Character e in batch 16\n",
      "Character f in batch 17\n",
      "Character w in batch 18\n",
      "Character i in batch 19\n",
      "Character y in batch 20\n",
      "Character n in batch 21\n",
      "Character o in batch 22\n",
      "Character   in batch 23\n",
      "Character c in batch 24\n",
      "Character   in batch 25\n",
      "Character c in batch 26\n",
      "Character t in batch 27\n",
      "Character p in batch 28\n",
      "Character e in batch 29\n",
      "Character u in batch 30\n",
      "Character c in batch 31\n",
      "Character d in batch 32\n",
      "Character   in batch 33\n",
      "Character e in batch 34\n",
      "Character s in batch 35\n",
      "Character l in batch 36\n",
      "Character t in batch 37\n",
      "Character i in batch 38\n",
      "Character s in batch 39\n",
      "Character s in batch 40\n",
      "Character i in batch 41\n",
      "Character o in batch 42\n",
      "Character e in batch 43\n",
      "Character   in batch 44\n",
      "Character t in batch 45\n",
      "Character a in batch 46\n",
      "Character p in batch 47\n",
      "Character   in batch 48\n",
      "Character   in batch 49\n",
      "Character e in batch 50\n",
      "Character c in batch 51\n",
      "Character h in batch 52\n",
      "Character e in batch 53\n",
      "Character n in batch 54\n",
      "Character n in batch 55\n",
      "Character   in batch 56\n",
      "Character c in batch 57\n",
      "Character n in batch 58\n",
      "Character a in batch 59\n",
      "Character e in batch 60\n",
      "Character i in batch 61\n",
      "Character p in batch 62\n",
      "Character   in batch 63\n",
      "Character   in batch 0\n",
      "Character n in batch 1\n",
      "Character r in batch 2\n",
      "Character b in batch 3\n",
      "Character r in batch 4\n",
      "Character   in batch 5\n",
      "Character n in batch 6\n",
      "Character o in batch 7\n",
      "Character n in batch 8\n",
      "Character r in batch 9\n",
      "Character   in batch 10\n",
      "Character b in batch 11\n",
      "Character i in batch 12\n",
      "Character r in batch 13\n",
      "Character e in batch 14\n",
      "Character   in batch 15\n",
      "Character   in batch 16\n",
      "Character i in batch 17\n",
      "Character o in batch 18\n",
      "Character s in batch 19\n",
      "Character   in batch 20\n",
      "Character d in batch 21\n",
      "Character n in batch 22\n",
      "Character t in batch 23\n",
      "Character e in batch 24\n",
      "Character i in batch 25\n",
      "Character o in batch 26\n",
      "Character   in batch 27\n",
      "Character a in batch 28\n",
      "Character r in batch 29\n",
      "Character s in batch 30\n",
      "Character a in batch 31\n",
      "Character u in batch 32\n",
      "Character a in batch 33\n",
      "Character   in batch 34\n",
      "Character s in batch 35\n",
      "Character   in batch 36\n",
      "Character   in batch 37\n",
      "Character m in batch 38\n",
      "Character t in batch 39\n",
      "Character   in batch 40\n",
      "Character s in batch 41\n",
      "Character s in batch 42\n",
      "Character i in batch 43\n",
      "Character i in batch 44\n",
      "Character h in batch 45\n",
      "Character h in batch 46\n",
      "Character r in batch 47\n",
      "Character b in batch 48\n",
      "Character i in batch 49\n",
      "Character   in batch 50\n",
      "Character h in batch 51\n",
      "Character a in batch 52\n",
      "Character d in batch 53\n",
      "Character g in batch 54\n",
      "Character e in batch 55\n",
      "Character r in batch 56\n",
      "Character y in batch 57\n",
      "Character s in batch 58\n",
      "Character t in batch 59\n",
      "Character e in batch 60\n",
      "Character o in batch 61\n",
      "Character e in batch 62\n",
      "Character h in batch 63\n",
      "Character a in batch 0\n",
      "Character   in batch 1\n",
      "Character i in batch 2\n",
      "Character e in batch 3\n",
      "Character i in batch 4\n",
      "Character a in batch 5\n",
      "Character d in batch 6\n",
      "Character p in batch 7\n",
      "Character   in batch 8\n",
      "Character a in batch 9\n",
      "Character y in batch 10\n",
      "Character o in batch 11\n",
      "Character s in batch 12\n",
      "Character   in batch 13\n",
      "Character   in batch 14\n",
      "Character w in batch 15\n",
      "Character s in batch 16\n",
      "Character e in batch 17\n",
      "Character   in batch 18\n",
      "Character t in batch 19\n",
      "Character c in batch 20\n",
      "Character   in batch 21\n",
      "Character   in batch 22\n",
      "Character o in batch 23\n",
      "Character r in batch 24\n",
      "Character t in batch 25\n",
      "Character n in batch 26\n",
      "Character t in batch 27\n",
      "Character i in batch 28\n",
      "Character   in batch 29\n",
      "Character   in batch 30\n",
      "Character p in batch 31\n",
      "Character p in batch 32\n",
      "Character n in batch 33\n",
      "Character j in batch 34\n",
      "Character   in batch 35\n",
      "Character t in batch 36\n",
      "Character i in batch 37\n",
      "Character e in batch 38\n",
      "Character   in batch 39\n",
      "Character s in batch 40\n",
      "Character   in batch 41\n",
      "Character c in batch 42\n",
      "Character g in batch 43\n",
      "Character t in batch 44\n",
      "Character e in batch 45\n",
      "Character o in batch 46\n",
      "Character i in batch 47\n",
      "Character e in batch 48\n",
      "Character n in batch 49\n",
      "Character f in batch 50\n",
      "Character y in batch 51\n",
      "Character r in batch 52\n",
      "Character   in batch 53\n",
      "Character   in batch 54\n",
      "Character o in batch 55\n",
      "Character i in batch 56\n",
      "Character c in batch 57\n",
      "Character e in batch 58\n",
      "Character i in batch 59\n",
      "Character t in batch 60\n",
      "Character n in batch 61\n",
      "Character a in batch 62\n",
      "Character a in batch 63\n",
      "Character n in batch 0\n",
      "Character m in batch 1\n",
      "Character a in batch 2\n",
      "Character y in batch 3\n",
      "Character e in batch 4\n",
      "Character n in batch 5\n",
      "Character   in batch 6\n",
      "Character e in batch 7\n",
      "Character f in batch 8\n",
      "Character t in batch 9\n",
      "Character o in batch 10\n",
      "Character e in batch 11\n",
      "Character t in batch 12\n",
      "Character h in batch 13\n",
      "Character m in batch 14\n",
      "Character h in batch 15\n",
      "Character i in batch 16\n",
      "Character r in batch 17\n",
      "Character s in batch 18\n",
      "Character o in batch 19\n",
      "Character a in batch 20\n",
      "Character i in batch 21\n",
      "Character o in batch 22\n",
      "Character   in batch 23\n",
      "Character t in batch 24\n",
      "Character   in batch 25\n",
      "Character v in batch 26\n",
      "Character o in batch 27\n",
      "Character g in batch 28\n",
      "Character s in batch 29\n",
      "Character t in batch 30\n",
      "Character i in batch 31\n",
      "Character l in batch 32\n",
      "Character n in batch 33\n",
      "Character a in batch 34\n",
      "Character z in batch 35\n",
      "Character h in batch 36\n",
      "Character n in batch 37\n",
      "Character n in batch 38\n",
      "Character h in batch 39\n",
      "Character u in batch 40\n",
      "Character s in batch 41\n",
      "Character i in batch 42\n",
      "Character h in batch 43\n",
      "Character a in batch 44\n",
      "Character   in batch 45\n",
      "Character m in batch 46\n",
      "Character s in batch 47\n",
      "Character c in batch 48\n",
      "Character   in batch 49\n",
      "Character a in batch 50\n",
      "Character   in batch 51\n",
      "Character m in batch 52\n",
      "Character e in batch 53\n",
      "Character i in batch 54\n",
      "Character   in batch 55\n",
      "Character s in batch 56\n",
      "Character l in batch 57\n",
      "Character   in batch 58\n",
      "Character n in batch 59\n",
      "Character   in batch 60\n",
      "Character s in batch 61\n",
      "Character l in batch 62\n",
      "Character v in batch 63\n",
      "Character a in batch 0\n",
      "Character i in batch 1\n",
      "Character   in batch 2\n",
      "Character s in batch 3\n",
      "Character d in batch 4\n",
      "Character d in batch 5\n",
      "Character l in batch 6\n",
      "Character n in batch 7\n",
      "Character r in batch 8\n",
      "Character i in batch 9\n",
      "Character r in batch 10\n",
      "Character i in batch 11\n",
      "Character e in batch 12\n",
      "Character a in batch 13\n",
      "Character a in batch 14\n",
      "Character o in batch 15\n",
      "Character g in batch 16\n",
      "Character c in batch 17\n",
      "Character i in batch 18\n",
      "Character t in batch 19\n",
      "Character n in batch 20\n",
      "Character n in batch 21\n",
      "Character f in batch 22\n",
      "Character p in batch 23\n",
      "Character a in batch 24\n",
      "Character w in batch 25\n",
      "Character i in batch 26\n",
      "Character l in batch 27\n",
      "Character n in batch 28\n",
      "Character i in batch 29\n",
      "Character e in batch 30\n",
      "Character t in batch 31\n",
      "Character i in batch 32\n",
      "Character   in batch 33\n",
      "Character n in batch 34\n",
      "Character e in batch 35\n",
      "Character e in batch 36\n",
      "Character s in batch 37\n",
      "Character s in batch 38\n",
      "Character o in batch 39\n",
      "Character p in batch 40\n",
      "Character t in batch 41\n",
      "Character l in batch 42\n",
      "Character t in batch 43\n",
      "Character l in batch 44\n",
      "Character t in batch 45\n",
      "Character a in batch 46\n",
      "Character e in batch 47\n",
      "Character o in batch 48\n",
      "Character a in batch 49\n",
      "Character b in batch 50\n",
      "Character t in batch 51\n",
      "Character a in batch 52\n",
      "Character m in batch 53\n",
      "Character n in batch 54\n",
      "Character l in batch 55\n",
      "Character k in batch 56\n",
      "Character o in batch 57\n",
      "Character t in batch 58\n",
      "Character g in batch 59\n",
      "Character g in batch 60\n",
      "Character   in batch 61\n",
      "Character   in batch 62\n",
      "Character e in batch 63\n",
      "Character r in batch 0\n",
      "Character l in batch 1\n",
      "Character a in batch 2\n",
      "Character   in batch 3\n",
      "Character   in batch 4\n",
      "Character   in batch 5\n",
      "Character i in batch 6\n",
      "Character e in batch 7\n",
      "Character o in batch 8\n",
      "Character o in batch 9\n",
      "Character k in batch 10\n",
      "Character n in batch 11\n",
      "Character d in batch 12\n",
      "Character s in batch 13\n",
      "Character d in batch 14\n",
      "Character   in batch 15\n",
      "Character n in batch 16\n",
      "Character e in batch 17\n",
      "Character x in batch 18\n",
      "Character l in batch 19\n",
      "Character   in batch 20\n",
      "Character t in batch 21\n",
      "Character   in batch 22\n",
      "Character a in batch 23\n",
      "Character i in batch 24\n",
      "Character i in batch 25\n",
      "Character n in batch 26\n",
      "Character d in batch 27\n",
      "Character   in batch 28\n",
      "Character d in batch 29\n",
      "Character x in batch 30\n",
      "Character a in batch 31\n",
      "Character c in batch 32\n",
      "Character e in batch 33\n",
      "Character u in batch 34\n",
      "Character r in batch 35\n",
      "Character o in batch 36\n",
      "Character t in batch 37\n",
      "Character i in batch 38\n",
      "Character l in batch 39\n",
      "Character p in batch 40\n",
      "Character i in batch 41\n",
      "Character l in batch 42\n",
      "Character   in batch 43\n",
      "Character y in batch 44\n",
      "Character o in batch 45\n",
      "Character   in batch 46\n",
      "Character   in batch 47\n",
      "Character m in batch 48\n",
      "Character   in batch 49\n",
      "Character i in batch 50\n",
      "Character o in batch 51\n",
      "Character n in batch 52\n",
      "Character p in batch 53\n",
      "Character   in batch 54\n",
      "Character a in batch 55\n",
      "Character y in batch 56\n",
      "Character p in batch 57\n",
      "Character h in batch 58\n",
      "Character   in batch 59\n",
      "Character r in batch 60\n",
      "Character m in batch 61\n",
      "Character o in batch 62\n",
      "Character   in batch 63\n",
      "Character c in batch 0\n",
      "Character i in batch 1\n",
      "Character r in batch 2\n",
      "Character a in batch 3\n",
      "Character u in batch 4\n",
      "Character r in batch 5\n",
      "Character t in batch 6\n",
      "Character d in batch 7\n",
      "Character m in batch 8\n",
      "Character n in batch 9\n",
      "Character   in batch 10\n",
      "Character g in batch 11\n",
      "Character   in batch 12\n",
      "Character   in batch 13\n",
      "Character e in batch 14\n",
      "Character r in batch 15\n",
      "Character i in batch 16\n",
      "Character   in batch 17\n",
      "Character   in batch 18\n",
      "Character e in batch 19\n",
      "Character b in batch 20\n",
      "Character r in batch 21\n",
      "Character t in batch 22\n",
      "Character s in batch 23\n",
      "Character n in batch 24\n",
      "Character l in batch 25\n",
      "Character c in batch 26\n",
      "Character   in batch 27\n",
      "Character a in batch 28\n",
      "Character e in batch 29\n",
      "Character t in batch 30\n",
      "Character l in batch 31\n",
      "Character a in batch 32\n",
      "Character s in batch 33\n",
      "Character a in batch 34\n",
      "Character o in batch 35\n",
      "Character r in batch 36\n",
      "Character a in batch 37\n",
      "Character o in batch 38\n",
      "Character y in batch 39\n",
      "Character o in batch 40\n",
      "Character l in batch 41\n",
      "Character a in batch 42\n",
      "Character s in batch 43\n",
      "Character   in batch 44\n",
      "Character w in batch 45\n",
      "Character p in batch 46\n",
      "Character l in batch 47\n",
      "Character e in batch 48\n",
      "Character n in batch 49\n",
      "Character a in batch 50\n",
      "Character   in batch 51\n",
      "Character   in batch 52\n",
      "Character e in batch 53\n",
      "Character p in batch 54\n",
      "Character t in batch 55\n",
      "Character   in batch 56\n",
      "Character e in batch 57\n",
      "Character e in batch 58\n",
      "Character f in batch 59\n",
      "Character i in batch 60\n",
      "Character o in batch 61\n",
      "Character f in batch 62\n",
      "Character m in batch 63\n",
      "Character h in batch 0\n",
      "Character t in batch 1\n",
      "Character c in batch 2\n",
      "Character n in batch 3\n",
      "Character r in batch 4\n",
      "Character i in batch 5\n",
      "Character u in batch 6\n",
      "Character   in batch 7\n",
      "Character   in batch 8\n",
      "Character   in batch 9\n",
      "Character o in batch 10\n",
      "Character   in batch 11\n",
      "Character w in batch 12\n",
      "Character p in batch 13\n",
      "Character   in batch 14\n",
      "Character e in batch 15\n",
      "Character f in batch 16\n",
      "Character c in batch 17\n",
      "Character e in batch 18\n",
      "Character   in batch 19\n",
      "Character e in batch 20\n",
      "Character a in batch 21\n",
      "Character h in batch 22\n",
      "Character s in batch 23\n",
      "Character   in batch 24\n",
      "Character l in batch 25\n",
      "Character e in batch 26\n",
      "Character h in batch 27\n",
      "Character n in batch 28\n",
      "Character   in batch 29\n",
      "Character s in batch 30\n",
      "Character i in batch 31\n",
      "Character t in batch 32\n",
      "Character   in batch 33\n",
      "Character r in batch 34\n",
      "Character   in batch 35\n",
      "Character i in batch 36\n",
      "Character n in batch 37\n",
      "Character n in batch 38\n",
      "Character   in batch 39\n",
      "Character r in batch 40\n",
      "Character l in batch 41\n",
      "Character t in batch 42\n",
      "Character u in batch 43\n",
      "Character l in batch 44\n",
      "Character e in batch 45\n",
      "Character r in batch 46\n",
      "Character i in batch 47\n",
      "Character s in batch 48\n",
      "Character a in batch 49\n",
      "Character n in batch 50\n",
      "Character r in batch 51\n",
      "Character n in batch 52\n",
      "Character r in batch 53\n",
      "Character o in batch 54\n",
      "Character i in batch 55\n",
      "Character r in batch 56\n",
      "Character d in batch 57\n",
      "Character   in batch 58\n",
      "Character r in batch 59\n",
      "Character d in batch 60\n",
      "Character r in batch 61\n",
      "Character   in batch 62\n",
      "Character a in batch 63\n",
      "Character i in batch 0\n",
      "Character a in batch 1\n",
      "Character h in batch 2\n",
      "Character d in batch 3\n",
      "Character r in batch 4\n",
      "Character c in batch 5\n",
      "Character r in batch 6\n",
      "Character f in batch 7\n",
      "Character t in batch 8\n",
      "Character t in batch 9\n",
      "Character t in batch 10\n",
      "Character s in batch 11\n",
      "Character i in batch 12\n",
      "Character r in batch 13\n",
      "Character t in batch 14\n",
      "Character c in batch 15\n",
      "Character i in batch 16\n",
      "Character r in batch 17\n",
      "Character i in batch 18\n",
      "Character s in batch 19\n",
      "Character   in batch 20\n",
      "Character c in batch 21\n",
      "Character e in batch 22\n",
      "Character   in batch 23\n",
      "Character d in batch 24\n",
      "Character   in batch 25\n",
      "Character   in batch 26\n",
      "Character i in batch 27\n",
      "Character d in batch 28\n",
      "Character s in batch 29\n",
      "Character   in batch 30\n",
      "Character z in batch 31\n",
      "Character e in batch 32\n",
      "Character d in batch 33\n",
      "Character y in batch 34\n",
      "Character t in batch 35\n",
      "Character e in batch 36\n",
      "Character c in batch 37\n",
      "Character a in batch 38\n",
      "Character m in batch 39\n",
      "Character t in batch 40\n",
      "Character   in batch 41\n",
      "Character i in batch 42\n",
      "Character b in batch 43\n",
      "Character a in batch 44\n",
      "Character r in batch 45\n",
      "Character e in batch 46\n",
      "Character n in batch 47\n",
      "Character   in batch 48\n",
      "Character z in batch 49\n",
      "Character   in batch 50\n",
      "Character e in batch 51\n",
      "Character e in batch 52\n",
      "Character o in batch 53\n",
      "Character l in batch 54\n",
      "Character n in batch 55\n",
      "Character i in batch 56\n",
      "Character i in batch 57\n",
      "Character a in batch 58\n",
      "Character o in batch 59\n",
      "Character   in batch 60\n",
      "Character e in batch 61\n",
      "Character d in batch 62\n",
      "Character d in batch 63\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      #print(\"Character %s in batch %d\" % (self._text[self._cursor[b]],b))\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1] # this is where we are adding character from previous batches to subsequent.\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) #print everything.\n",
    "\n",
    "#print(batches2string(train_batches.next())[0])\n",
    "#print(np.shape(train_batches.next()))\n",
    "\n",
    "#VD: batch is (11, 64, 27) and consists of:\n",
    "# - 11 - the last symbol of the previous array, followed by num_unrollings (10) new ones.\n",
    "# - 64 - is a number of batches, where 1 batch=strings (e.g. 'ons anarchi')\n",
    "# - 27 vocabulary size (english alphabet + \" \")\n",
    "\n",
    "#print(train_batches.next()[0])\n",
    "print(batches2string(train_batches.next()))\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (64, 27)\n",
      "Out shape: (64, 64)\n",
      "Input gate shape: (64, 64)\n",
      "Input shape: (1, 27)\n",
      "Out shape: (1, 64)\n",
      "Input gate shape: (1, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64 #Number of nodes in NN layers. It is equal to number of batches. I guess otherwise, it won't work.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings. \n",
    "  # VD: afaiu, unrolling is refered to horizon of memory applied to input\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    #VD let's breakdown operations.\n",
    "    #1. Input gate. add to current input previous output and bias. Run it through sigmoid.\n",
    "    #shapes: [batch_size,vocabulary_size]*[vocab_size, num_nodes] + [batch size, num nodes] + [1, num nodes]\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib) \n",
    "    #print(\"Input shape: %s\" % i.get_shape())\n",
    "    #print(\"Out shape: %s\" % o.get_shape())\n",
    "    #print(\"Input gate shape: %s\" % input_gate.get_shape())\n",
    "    \n",
    "    #2. Forget gate. add to current input previous output and bias. Run it through sigmoid.\n",
    "    #shapes: [batch_size,vocabulary_size]*[vocab_size, num_nodes] + [batch size, num nodes] + [1, num nodes]\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    \n",
    "    #3. Update or candidate. add to current input previous output and bias. Run it through sigmoid.\n",
    "    #shapes: [batch_size,vocabulary_size]*[vocab_size, num_nodes] + [batch size, num nodes] + [1, num nodes]\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    \n",
    "    #4. Here \"*\" is a pointwise multiplication and implemented filtering of previous state using sigmoid within (0,1)\n",
    "    # also tahn is withing (-1,1)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  # VD: tf.control_dependecies make sure that input Ops are executed before executing internal graph code.\n",
    "  # VD: tf.Variable.assign assigns a new value to the variable.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b) #VD: should be similar to tf.matmul(x, w) + b \n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25) # VD: clipping of the gradients to avoid explosion.\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # tf.group creats an op that groups multiple operations.\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298338 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "nhyphnhuelgnvelng abeniea l fegeantwqrcualbktugeenjoq   d lxkmf olgrnnpef afmmx \n",
      "dneetaascxcgqpnpczplvqjntliocoamhu aoc rufn mais pdex yutsfwidd wjrxrwphuecn f a\n",
      "qvvcodka m ovu  aoexedu m s tcsm iepzooddabftzcjnst ip nbjdrtnor ho oqdso   e re\n",
      "gtqas jd eousewkkomag fuznhhxxc h  civzww l zurat btlrdnexhxn rwnaziketodnemeeua\n",
      "pxsmr wyt sz wx caym c  ibadyjss p nqbtdsa qwtkhapsmam yezmpczmedgqnqrpft c xrt \n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.594232 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.75\n",
      "Validation set perplexity: 10.14\n",
      "Average loss at step 200: 2.248669 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 300: 2.091511 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 400: 1.994529 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 500: 1.933106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 600: 1.905983 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 700: 1.852655 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 800: 1.815561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.824251 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.819993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      " geament of a then preding receutee in semmed thes of a man ats were rezero to t\n",
      "ch and de beenen condicrece was ia emesticis at wal bent mora woer the nevedy we\n",
      "y the be yeroudering of eilk emsus asoliste weald eight wus but moro faldhwated \n",
      " schiloun jaker ebersted hilly dicon hay three jealic sungized dibtle it hwar or\n",
      "s heackniton stranities wite of the ribch two of arman trelems pru and enoling o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1100: 1.772391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1200: 1.750068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.733713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1400: 1.746215 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1500: 1.738107 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1600: 1.746508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.712264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.672266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1900: 1.643962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2000: 1.694646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "quitatureen engrisuattery praphed a lead in he gengrish wed pering  both who the\n",
      "quess in is not earchi liquicding sinctod spase mosite and difenains and discong\n",
      "haled a hasta concite world mensic bridd uprassielly mborsting betwe polifed in \n",
      "litas tod and initioniked spased basiage a proquctions sincially werer gotsing z\n",
      "ix dosingury for them con ising s priscity computting has one nine eight zero ze\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2100: 1.684006 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2200: 1.680588 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2300: 1.640202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2400: 1.659117 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500: 1.680296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.651220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2700: 1.651747 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.649564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2900: 1.647462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3000: 1.649202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ecth in eque heightance ofcems it with firsh player wert some cill inguny with a\n",
      "s creyplets gerted foul octered in the of tippen foothners oreawity meir dickeli\n",
      "us one writerind of bela he illent of poasement sofiet hordhtan an two mecomed t\n",
      "chuman seven operal holds when two strunoan theopose hearls united garc of the r\n",
      "hi in starcelinnal wivem parate bequidmallyn augusti den mathera raska s rolds r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3100: 1.624283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.645290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.638156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400: 1.664816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.657325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.666287 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3700: 1.641889 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800: 1.640931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.633497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.646879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "play the freicss of reputary politically or sahies the renduectorom boldaip for \n",
      "ver s inclumisions brongi he stal roundent onry sell prime or and his statesment\n",
      "ing the ynostaving ncanaushn and hore it whiles in the massited in invleing to e\n",
      "d deptempt in musicured in voted indx holimindia mistics of led to the emperor t\n",
      "x avot s dzistured prictivite redicais compredice by the by undiedver gove in th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.630096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.631979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.614009 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4400: 1.603583 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.612281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4600: 1.612208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.622053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.626789 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.628645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.604150 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "verborge criti inbab writes dainw mainthas minet it psombential lacal mure the d\n",
      "d on the mask also provinge references belied riber just had a doment bso travi \n",
      "quent sceb the premetsophints eagus as as two zero zero zero as links leader mem\n",
      "smind by that doyas iramily that subgron as conembil at with the des and rathing\n",
      "jack much than exic one nine five five seven eworen used in the cogre ouraminify\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5100: 1.599058 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.587940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5300: 1.574454 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.577405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.566874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.581882 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.568655 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.581257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.574526 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.543566 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "used heiga more delawed two guiging for soviet kulting was of lew and najuation \n",
      "rightic sabpan the most wafe counary feftes gre c or parter sequosidept ad these\n",
      "ward cans of the kandraper greek vainstropenoi a desty the conventions as a maki\n",
      "lism adire kingtoguers all meadon ara worbshatanize mastarm the peruoacter proje\n",
      "phar of the new in chate was ail seosabt ime is suptrantly that seurning s with \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6100: 1.561012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.530691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.542546 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.539165 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.556689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.593192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.577659 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.600117 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.584140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7000: 1.578697 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "bake clast honguaguely unimelican pressiavle leath she chapia for the adiration \n",
      "ug is dows mideers shours of the dut bellies light officially that norment in pr\n",
      "el used agentod the mayno lije association and hurper was good the p intropetey \n",
      "jugins have fingle to numequarding benespable activiking thooob that tuigh the s\n",
      "zer restruced to allow petioned roekends of the visu lead which see the dequring\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#details here: https://discussions.udacity.com/t/deep-learning-assignment-6-problem-2/191791/19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
